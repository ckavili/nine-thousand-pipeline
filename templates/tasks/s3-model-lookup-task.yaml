apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: s3-model-lookup-task
spec:
  params:
    - name: S3_BUCKET
      description: S3 bucket name where data changed
      type: string
    - name: S3_OBJECT_KEY
      description: S3 object key that changed
      type: string
    - name: TARGET_PIPELINE
      description: Name of the pipeline to trigger for retraining
      type: string
      default: "ct-pipeline"
    - name: GIT_URL
      description: Git repository URL to pass to triggered pipelines
      type: string
  workspaces:
    - name: model-specs
      description: Workspace containing cloned model specifications
  results:
    - name: MODELS_TO_RETRAIN
      description: Comma-separated list of model names that need retraining
  steps:
    - name: lookup-models
      workingDir: $(workspaces.model-specs.path)
      image: registry.redhat.io/ubi9/python-311:latest
      script: |
        #!/bin/bash
        set -euo pipefail
        
        S3_BUCKET="$(params.S3_BUCKET)"
        S3_OBJECT_KEY="$(params.S3_OBJECT_KEY)"
        
        echo "S3 Event: Bucket=${S3_BUCKET}, Key=${S3_OBJECT_KEY}"
        
        # Find models that match the S3 data source
        models_to_retrain=""
        
        cd model-specs/nine-thousand-models
        for model_file in *.json; do
          if [ -f "$model_file" ]; then
            echo "Processing model file: $model_file"
            
            # Extract and analyze data_source from JSON
            python3 -c "
        import json
        import sys
        
        with open('$model_file', 'r') as f:
            data = json.load(f)
        
        data_source = data.get('data_source', {})
        
        # Handle both old string format and new structured format
        if isinstance(data_source, str):
            # Legacy format: simple string matching
            if data_source == '$S3_BUCKET' or '$S3_OBJECT_KEY'.find(data_source) >= 0:
                print('MATCH')
            else:
                print('NO_MATCH')
        elif isinstance(data_source, dict) and data_source.get('type') == 's3':
            # New structured format
            bucket = data_source.get('bucket', '')
            prefix = data_source.get('prefix', '')
            pattern = data_source.get('pattern', '*')
            
            print(f'Checking S3 data source: bucket={bucket}, prefix={prefix}, pattern={pattern}')
            
            # Check if S3 event matches this model's data source
            bucket_match = (bucket == '$S3_BUCKET')
            prefix_match = ('$S3_OBJECT_KEY'.startswith(prefix) if prefix else True)
            
            if bucket_match and prefix_match:
                print('MATCH')
            else:
                print('NO_MATCH')
        else:
            print('NO_MATCH')
        " > /tmp/match_result
            
            match_result=$(cat /tmp/match_result | tail -1)
            echo "Match result for $model_file: $match_result"
            
            if [ "$match_result" = "MATCH" ]; then
              model_name="${model_file%.json}"
              echo "Match found! Model $model_name needs retraining"
              if [ -z "$models_to_retrain" ]; then
                models_to_retrain="$model_name"
              else
                models_to_retrain="$models_to_retrain,$model_name"
              fi
            fi
          fi
        done
        
        echo "Models to retrain: $models_to_retrain"
        echo -n "$models_to_retrain" > $(results.MODELS_TO_RETRAIN.path)
        
    - name: trigger-retraining
      image: registry.redhat.io/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -euo pipefail
        
        # Download tkn CLI
        curl -sL https://mirror.openshift.com/pub/openshift-v4/clients/pipeline/latest/tkn-linux-amd64.tar.gz | tar --no-same-owner -xzf - -C /tmp tkn 
        chmod -R 755 /tmp/tkn
        
        MODELS_TO_RETRAIN=$(cat $(results.MODELS_TO_RETRAIN.path))
        
        if [ -z "$MODELS_TO_RETRAIN" ]; then
          echo "No models need retraining for this S3 change"
          exit 0
        fi
        
        echo "Triggering retraining for models: $MODELS_TO_RETRAIN"
        
        # Trigger model pipeline for each model in parallel
        IFS=',' read -ra MODEL_ARRAY <<< "$MODELS_TO_RETRAIN"
        for model in "${MODEL_ARRAY[@]}"; do
          echo "Triggering $(params.TARGET_PIPELINE) for model: $model"
          /tmp/tkn pipeline start $(params.TARGET_PIPELINE) --prefix-name s3-retrain-${model} \
            -p GIT_URL="$(params.GIT_URL)" \
            -p GIT_REPO_NAME="nine-thousand-models" \
            -p GIT_SHORT_REVISION="s3-trigger" \
            -p GIT_COMMIT_AUTHOR="s3-event" \
            -p MODEL_NAME="$model" \
            -p PROJECT_NAME="fika" \
            --workspace name=shared-workspace,claimName=shared-workspace \
            --workspace name=model-workspace,claimName=model-pvc \
            --showlog &
        done
        
        # Wait for all background jobs
        wait
        
        echo "All retraining pipelines triggered successfully"