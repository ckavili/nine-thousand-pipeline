---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: execute-ds-pipeline
spec:
  workspaces:
    - name: output
  params:
    - name: WORK_DIRECTORY
      description: Directory to start build in (handle multiple branches)
      type: string
    - name: MODEL_NAME
      description: Name of the model
      type: string
    - name: GIT_SHORT_REVISION
      description: Short Git commit hast
      type: string
  results:
    - name: KFP_RUN_ID
      description: Run ID of the Data Science Pipeline
  steps:
  - name: execute-ds-pipeline
    workingDir: $(workspaces.output.path)/$(params.WORK_DIRECTORY)
    image: registry.redhat.io/ubi9/python-311@sha256:fc669a67a0ef9016c3376b2851050580b3519affd5ec645d629fd52d2a8b8e4a
    command: ["/bin/sh", "-c"]
    args:
    - |
      echo "Executing Data Science Pipeline for model: $(params.MODEL_NAME) and version: $(params.GIT_SHORT_REVISION)"
      echo "Working directory: $(workspaces.output.path)/$(params.WORK_DIRECTORY)"
      ls -l 
      cat $(params.MODEL_NAME).json

      python3 -m pip install kfp.kubernetes==1.3.0 kfp==2.9.0
      cat << 'EOF' | python3
      import kfp
      import json

      # Load pipeline metadata
      with open('$(params.MODEL_NAME).json') as f:
          pipeline_metadata = json.load(f)

      """
      pipeline metadata structure:
      {
        "data_source": {
            "type": "s3",
            "bucket": "data",
            "prefix": "",
            "pattern": "*.parquet"
          },
        "model_name": "window-predictor-model",
        "kubeflow_pipeline": "parts-predictor-pipeline",
        "kubeflow_pipeline_version": "parts-predictor-pipeline-v1-8-0",
        "runtime_name": "window-predictor-runtime",
        "model_hyperparameters": {
          "epochs": 2
        }
      }
      """  

      data_source = pipeline_metadata.get("data_source", {})
      model_name = "$(params.MODEL_NAME)"
      kubeflow_pipeline = pipeline_metadata.get("kubeflow_pipeline", "")
      kubeflow_pipeline_version = pipeline_metadata.get("kubeflow_pipeline_version", "")
      model_hyperparameters = pipeline_metadata.get("model_hyperparameters", {})

      namespace_file_path =\
          '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
      with open(namespace_file_path, 'r') as namespace_file:
          namespace = namespace_file.read()

      kubeflow_endpoint =\
          f'https://ds-pipeline-dspa.{namespace}.svc:8443'

      sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
      with open(sa_token_file_path, 'r') as token_file:
          bearer_token = token_file.read()

      ssl_ca_cert =\
          '/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt'

      print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
      client = kfp.Client(
          host=kubeflow_endpoint,
          existing_token=bearer_token,
          ssl_ca_cert=ssl_ca_cert
      )

      # Get the pipeline ID
      kubeflow_pipeline_id = ""
      for pipeline in client.list_pipelines().pipelines:
        if pipeline.display_name == kubeflow_pipeline:
          kubeflow_pipeline_id = pipeline.pipeline_id

      if not kubeflow_pipeline_id:
        raise Exception(f"Could not find the kubeflow pipeline: {kubeflow_pipeline}")
      else:
        print(f"Found kubeflow pipeline: {kubeflow_pipeline} with id: {kubeflow_pipeline_id}")

      # Get the version ID
      kubeflow_pipeline_version_id = ""
      versions = client.list_pipeline_versions(kubeflow_pipeline_id, filter=json.dumps({
          "predicates": [{
              "operation": "IS_SUBSTRING",
              "key": "display_name",
              "stringValue": kubeflow_pipeline_version
          }]
      })).pipeline_versions

      if len(versions) > 1:
        for version in versions:
          if version.display_name == kubeflow_pipeline_version:
            kubeflow_pipeline_version_id = version.pipeline_version_id
            print(f"Found kubeflow pipeline version: {version.display_name} with id: {kubeflow_pipeline_version_id}")

      if versions is None:
        raise Exception(f"Could not find the kubeflow pipeline version: {kubeflow_pipeline_version}")
      elif kubeflow_pipeline_version_id!="":
        pass
      else:
        kubeflow_pipeline_version_id = versions[0].pipeline_version_id
        print(f"Found kubeflow pipeline version: {versions[0].display_name} with id: {kubeflow_pipeline_version_id}")
      
      default_parameters = {}
      default_parameters['version'] = "$(params.GIT_SHORT_REVISION)"
      default_parameters['model_name'] = model_name
      default_parameters['model_storage_pvc'] = model_name+"-model"
      default_parameters['cluster_domain'] = "{{ .Values.cluster_domain }}"
      default_parameters['prod_flag'] = True
      default_parameters['hyperparameters'] = model_hyperparameters
      default_parameters['data_source'] = data_source

      # start a run
      print("üèÉ‚Äç‚ôÇÔ∏è start a run")
      print(f"Starting with parameters: {default_parameters}")
      run_id = client.run_pipeline(
        experiment_id="", 
        job_name=model_name+"-run", 
        pipeline_id=kubeflow_pipeline_id, 
        version_id=kubeflow_pipeline_version_id,
        params=default_parameters
      )

      print("ü•± wait for the run to finish")
      # wait for the run to finish
      client.wait_for_run_completion(
          run_id=run_id.run_id, 
          timeout=7200,
          sleep_duration=5,
      )
      
      # save run id to store it in model metadata for tracing
      KFP_RUN_ID = run_id.run_id
      path = "$(results.KFP_RUN_ID.path)"

      with open(path, "w") as file:
          file.write(KFP_RUN_ID)

      print("üéâ job finished üôå")
      EOF

